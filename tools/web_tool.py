"""
ç½‘é¡µå·¥å…·
æ”¯æŒç½‘é¡µå†…å®¹æå–ã€APIè°ƒç”¨ã€æ•°æ®æŠ“å–
"""
import requests
from bs4 import BeautifulSoup
from typing import Dict, Any, List, Optional
import json
from . import BaseTool

class WebTool(BaseTool):
    """ç½‘é¡µå·¥å…·"""
    
    def __init__(self):
        super().__init__(
            name="web_tool",
            description="""ç½‘é¡µç›¸å…³æ“ä½œå·¥å…·ã€‚æ”¯æŒï¼š
1. è·å–ç½‘é¡µå†…å®¹: æŠ“å–ç½‘é¡µæ–‡æœ¬å†…å®¹
2. æå–é“¾æ¥: æå–ç½‘é¡µä¸­æ‰€æœ‰é“¾æ¥
3. æå–å›¾ç‰‡: æå–ç½‘é¡µä¸­æ‰€æœ‰å›¾ç‰‡
4. è°ƒç”¨API: è°ƒç”¨RESTful APIæ¥å£
5. è§£æJSON: è§£æJSONæ ¼å¼æ•°æ®
6. ç¤ºä¾‹: "è·å–https://example.comå†…å®¹", "æå–é¡µé¢é“¾æ¥", "è°ƒç”¨APIè·å–æ•°æ®"
"""
        )
        
        # è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        
        # è¶…æ—¶è®¾ç½®
        self.timeout = 10
    
    def validate_input(self, input_data: Dict[str, Any]) -> bool:
        """éªŒè¯è¾“å…¥"""
        operation = input_data.get("operation", "")
        url = input_data.get("url", "")
        
        if not operation or not isinstance(operation, str):
            return False
        
        # å®‰å…¨æ£€æŸ¥
        if url:
            # æ£€æŸ¥URLæ ¼å¼
            if not url.startswith(('http://', 'https://')):
                return False
            
            # æ£€æŸ¥å±é™©åŸŸå
            dangerous_domains = ['localhost', '127.0.0.1', '192.168.', '10.']
            if any(domain in url for domain in dangerous_domains):
                return False
        
        return True
    
    def execute(self, operation: str, url: str = "", **kwargs) -> str:
        """æ‰§è¡Œç½‘é¡µæ“ä½œ"""
        try:
            operation = operation.lower().strip()
            
            if operation in ["fetch", "è·å–", "æŠ“å–"]:
                return self._fetch_webpage(url)
            
            elif operation in ["links", "é“¾æ¥", "æå–é“¾æ¥"]:
                return self._extract_links(url)
            
            elif operation in ["images", "å›¾ç‰‡", "æå–å›¾ç‰‡"]:
                return self._extract_images(url)
            
            elif operation in ["api", "è°ƒç”¨api"]:
                method = kwargs.get("method", "GET")
                data = kwargs.get("data", {})
                return self._call_api(url, method, data)
            
            elif operation in ["parse_json", "è§£æjson"]:
                json_str = kwargs.get("json_str", "")
                return self._parse_json(json_str)
            
            elif operation in ["help", "å¸®åŠ©"]:
                return self._get_help()
            
            else:
                return f"ä¸æ”¯æŒçš„æ“ä½œ: {operation}\n{self._get_help()}"
                
        except Exception as e:
            return f"ç½‘é¡µæ“ä½œé”™è¯¯: {str(e)}"
    
    def _fetch_webpage(self, url: str) -> str:
        """è·å–ç½‘é¡µå†…å®¹"""
        try:
            response = requests.get(
                url,
                headers=self.headers,
                timeout=self.timeout,
                verify=True  # éªŒè¯SSLè¯ä¹¦
            )
            
            response.raise_for_status()
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style"]):
                script.decompose()
            
            # è·å–æ–‡æœ¬
            text = soup.get_text()
            
            # æ¸…ç†æ–‡æœ¬
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = '\n'.join(chunk for chunk in chunks if chunk)
            
            # é™åˆ¶é•¿åº¦
            if len(text) > 2000:
                text = text[:2000] + "\n...(å†…å®¹æˆªæ–­ï¼Œåªæ˜¾ç¤ºå‰2000å­—ç¬¦)"
            
            # è·å–é¡µé¢ä¿¡æ¯
            title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
            
            result = [
                f"ğŸŒ ç½‘é¡µ: {url}",
                f"ğŸ“„ æ ‡é¢˜: {title}",
                f"ğŸ“Š çŠ¶æ€: {response.status_code}",
                f"ğŸ“ é•¿åº¦: {len(response.text)} å­—ç¬¦",
                f"\nğŸ“ å†…å®¹æ‘˜è¦:\n{text}"
            ]
            
            return "\n".join(result)
            
        except requests.exceptions.Timeout:
            return f"è¯·æ±‚è¶…æ—¶: {url}"
        except requests.exceptions.HTTPError as e:
            return f"HTTPé”™è¯¯: {e.response.status_code} - {url}"
        except requests.exceptions.ConnectionError:
            return f"è¿æ¥é”™è¯¯: {url}"
        except requests.exceptions.RequestException as e:
            return f"è¯·æ±‚é”™è¯¯: {str(e)}"
        except Exception as e:
            return f"è§£æé”™è¯¯: {str(e)}"
    
    def _extract_links(self, url: str) -> str:
        """æå–é“¾æ¥"""
        try:
            response = requests.get(
                url,
                headers=self.headers,
                timeout=self.timeout
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ‰€æœ‰é“¾æ¥
            links = []
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                text = link.get_text(strip=True)
                
                # å¤„ç†ç›¸å¯¹é“¾æ¥
                if href.startswith('/'):
                    href = requests.compat.urljoin(url, href)
                elif not href.startswith(('http://', 'https://')):
                    continue
                
                if text:
                    links.append(f"{text}: {href}")
                else:
                    links.append(f"{href}")
            
            if not links:
                return "æœªæ‰¾åˆ°é“¾æ¥"
            
            result = [f"åœ¨ {url} ä¸­æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥:"]
            for i, link in enumerate(links[:10], 1):
                result.append(f"{i}. {link}")
            
            if len(links) > 10:
                result.append(f"... è¿˜æœ‰ {len(links) - 10} ä¸ªé“¾æ¥æœªæ˜¾ç¤º")
            
            return "\n".join(result)
            
        except Exception as e:
            return f"æå–é“¾æ¥å¤±è´¥: {str(e)}"
    
    def _extract_images(self, url: str) -> str:
        """æå–å›¾ç‰‡"""
        try:
            response = requests.get(
                url,
                headers=self.headers,
                timeout=self.timeout
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ‰€æœ‰å›¾ç‰‡
            images = []
            for img in soup.find_all('img', src=True):
                src = img.get('src')
                alt = img.get('alt', 'æ— æè¿°')
                
                # å¤„ç†ç›¸å¯¹è·¯å¾„
                if src.startswith('/'):
                    src = requests.compat.urljoin(url, src)
                elif not src.startswith(('http://', 'https://', 'data:image')):
                    continue
                
                images.append(f"{alt}: {src}")
            
            if not images:
                return "æœªæ‰¾åˆ°å›¾ç‰‡"
            
            result = [f"åœ¨ {url} ä¸­æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡:"]
            for i, img in enumerate(images[:5], 1):
                result.append(f"{i}. {img}")
            
            if len(images) > 5:
                result.append(f"... è¿˜æœ‰ {len(images) - 5} å¼ å›¾ç‰‡æœªæ˜¾ç¤º")
            
            return "\n".join(result)
            
        except Exception as e:
            return f"æå–å›¾ç‰‡å¤±è´¥: {str(e)}"
    
    def _call_api(self, url: str, method: str = "GET", data: Dict = None) -> str:
        """è°ƒç”¨API"""
        try:
            method = method.upper()
            
            if method == "GET":
                response = requests.get(
                    url,
                    headers=self.headers,
                    timeout=self.timeout
                )
            elif method == "POST":
                response = requests.post(
                    url,
                    json=data,
                    headers=self.headers,
                    timeout=self.timeout
                )
            elif method == "PUT":
                response = requests.put(
                    url,
                    json=data,
                    headers=self.headers,
                    timeout=self.timeout
                )
            elif method == "DELETE":
                response = requests.delete(
                    url,
                    headers=self.headers,
                    timeout=self.timeout
                )
            else:
                return f"ä¸æ”¯æŒçš„HTTPæ–¹æ³•: {method}"
            
            response.raise_for_status()
            
            # è§£æå“åº”
            try:
                json_data = response.json()
                formatted = json.dumps(json_data, ensure_ascii=False, indent=2)
                
                # é™åˆ¶é•¿åº¦
                if len(formatted) > 2000:
                    formatted = formatted[:2000] + "\n...(JSONæˆªæ–­)"
                
                result = [
                    f"ğŸŒ API: {url}",
                    f"ğŸ“¤ æ–¹æ³•: {method}",
                    f"ğŸ“Š çŠ¶æ€: {response.status_code}",
                    f"\nğŸ“„ å“åº”:\n{formatted}"
                ]
                
                return "\n".join(result)
                
            except ValueError:
                # å¦‚æœä¸æ˜¯JSONï¼Œè¿”å›æ–‡æœ¬
                text = response.text[:1000]
                if len(response.text) > 1000:
                    text += "\n...(æ–‡æœ¬æˆªæ–­)"
                
                result = [
                    f"ğŸŒ API: {url}",
                    f"ğŸ“¤ æ–¹æ³•: {method}",
                    f"ğŸ“Š çŠ¶æ€: {response.status_code}",
                    f"\nğŸ“„ å“åº”:\n{text}"
                ]
                
                return "\n".join(result)
            
        except requests.exceptions.RequestException as e:
            return f"APIè°ƒç”¨å¤±è´¥: {str(e)}"
        except Exception as e:
            return f"APIå¤„ç†å¤±è´¥: {str(e)}"
    
    def _parse_json(self, json_str: str) -> str:
        """è§£æJSON"""
        try:
            data = json.loads(json_str)
            
            # æ ¼å¼åŒ–
            formatted = json.dumps(data, ensure_ascii=False, indent=2)
            
            # é™åˆ¶é•¿åº¦
            if len(formatted) > 2000:
                formatted = formatted[:2000] + "\n...(JSONæˆªæ–­)"
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = self._analyze_json(data)
            
            result = [
                "ğŸ“„ JSONè§£æç»“æœ:",
                f"ğŸ“Š ç»Ÿè®¡: {stats}",
                f"\nğŸ” å†…å®¹:\n{formatted}"
            ]
            
            return "\n".join(result)
            
        except json.JSONDecodeError as e:
            return f"JSONè§£æé”™è¯¯: {str(e)}"
        except Exception as e:
            return f"JSONå¤„ç†å¤±è´¥: {str(e)}"
    
    def _analyze_json(self, data: Any) -> str:
        """åˆ†æJSONç»“æ„"""
        if isinstance(data, dict):
            count = len(data)
            types = {}
            for key, value in data.items():
                t = type(value).__name__
                types[t] = types.get(t, 0) + 1
            
            type_str = ", ".join([f"{k}:{v}" for k, v in types.items()])
            return f"å¯¹è±¡ ({count}ä¸ªé”®), ç±»å‹: {type_str}"
        
        elif isinstance(data, list):
            count = len(data)
            if count > 0:
                sample = data[0]
                return f"æ•°ç»„ ({count}ä¸ªå…ƒç´ ), ç¤ºä¾‹ç±»å‹: {type(sample).__name__}"
            else:
                return f"ç©ºæ•°ç»„"
        
        else:
            return f"å€¼ç±»å‹: {type(data).__name__}"
    
    def _get_help(self) -> str:
        """è·å–å¸®åŠ©"""
        help_text = """
å¯ç”¨æ“ä½œ:
1. è·å–ç½‘é¡µ: operation="fetch", url="https://example.com"
2. æå–é“¾æ¥: operation="links", url="https://example.com"
3. æå–å›¾ç‰‡: operation="images", url="https://example.com"
4. è°ƒç”¨API: operation="api", url="https://api.example.com", method="GET", data={{}} (å¯é€‰)
5. è§£æJSON: operation="parse_json", json_str='{{"key": "value"}}'

ç¤ºä¾‹:
- è·å–ç½‘é¡µ: operation="fetch", url="https://example.com"
- è°ƒç”¨API: operation="api", url="https://api.example.com/data", method="GET"
- è§£æJSON: operation="parse_json", json_str='{{"name": "test"}}'
"""
        return help_text

# åˆ›å»ºå·¥å…·å®ä¾‹
web_tool = WebTool()